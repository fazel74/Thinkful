{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img = []\n",
    "train_label = []\n",
    "for path in glob.glob(\"../Fazel/fruits-360/Training/*\"):\n",
    "    img_label = path.split(\"/\")[-1]\n",
    "    for image in glob.glob(os.path.join(path, \"*.jpg\")):\n",
    "        img = cv2.imread(image) # the order of colors is BGR when an image file is read by imread()\n",
    "        img = cv2.resize(img, (64, 64)) # size reduction for purpose of having enough memory\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # convert to RGB because the order of colors is assumed to be RGB\n",
    "        train_img.append(img)\n",
    "        train_label.append(img_label)\n",
    "train_img = np.array(train_img)\n",
    "train_label = np.array(train_label)\n",
    "len(np.unique(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = []\n",
    "test_label =[]\n",
    "for path in glob.glob(\"../Fazel/fruits-360/Test/*\"):\n",
    "    img_label = path.split(\"/\")[-1]\n",
    "    for image in glob.glob(os.path.join(path, \"*.jpg\")):\n",
    "        img = cv2.imread(image) # the order of colors is BGR when an image file is read by imread()\n",
    "        img = cv2.resize(img, (64, 64)) # size reduction for purpose of having enough memory\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # convert to RGB because the order of colors is assumed to be RGB\n",
    "        test_img.append(img)\n",
    "        test_label.append(img_label)\n",
    "test_img = np.array(test_img)\n",
    "test_label = np.array(test_label)\n",
    "len(np.unique(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29586, 64, 64, 3)\n",
      "(10102, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_img.shape)\n",
    "print(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Training\\\\Apple Braeburn', 1: 'Training\\\\Apple Crimson Snow', 2: 'Training\\\\Apple Golden 1', 3: 'Training\\\\Apple Granny Smith', 4: 'Training\\\\Apple Pink Lady', 5: 'Training\\\\Apple Red 1', 6: 'Training\\\\Apple Red Yellow 1', 7: 'Training\\\\Apricot', 8: 'Training\\\\Avocado', 9: 'Training\\\\Avocado ripe', 10: 'Training\\\\Banana', 11: 'Training\\\\Beetroot', 12: 'Training\\\\Blueberry', 13: 'Training\\\\Cantaloupe 1', 14: 'Training\\\\Cauliflower', 15: 'Training\\\\Cherry 1', 16: 'Training\\\\Chestnut', 17: 'Training\\\\Clementine', 18: 'Training\\\\Cocos', 19: 'Training\\\\Dates', 20: 'Training\\\\Eggplant', 21: 'Training\\\\Ginger Root', 22: 'Training\\\\Grape Blue', 23: 'Training\\\\Grape Pink', 24: 'Training\\\\Grape White', 25: 'Training\\\\Grapefruit White', 26: 'Training\\\\Hazelnut', 27: 'Training\\\\Huckleberry', 28: 'Training\\\\Kaki', 29: 'Training\\\\Kiwi', 30: 'Training\\\\Kumquats', 31: 'Training\\\\Lemon', 32: 'Training\\\\Limes', 33: 'Training\\\\Mandarine', 34: 'Training\\\\Mango', 35: 'Training\\\\Mango Red', 36: 'Training\\\\Mulberry', 37: 'Training\\\\Nectarine', 38: 'Training\\\\Nut Pecan', 39: 'Training\\\\Onion Red', 40: 'Training\\\\Onion White', 41: 'Training\\\\Orange', 42: 'Training\\\\Papaya', 43: 'Training\\\\Passion Fruit', 44: 'Training\\\\Peach', 45: 'Training\\\\Pear', 46: 'Training\\\\Pepper Green', 47: 'Training\\\\Pepper Red', 48: 'Training\\\\Pepper Yellow', 49: 'Training\\\\Pineapple', 50: 'Training\\\\Plum', 51: 'Training\\\\Pomegranate', 52: 'Training\\\\Potato Red', 53: 'Training\\\\Potato Sweet', 54: 'Training\\\\Potato White', 55: 'Training\\\\Raspberry', 56: 'Training\\\\Tangelo', 57: 'Training\\\\Tomato 1', 58: 'Training\\\\Tomato Cherry Red', 59: 'Training\\\\Walnut'}\n"
     ]
    }
   ],
   "source": [
    "# Label train set\n",
    "id = {v:k for k,v in enumerate(np.unique(train_label)) }\n",
    "target = {v:k for k,v in id.items() }\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ... 59 59 59]\n",
      "(29586, 64, 64, 3) (29586,)\n"
     ]
    }
   ],
   "source": [
    "# Place train labels in a numpy array\n",
    "label_id = np.array([id[i] for i in train_label])\n",
    "print(label_id)\n",
    "print(train_img.shape, label_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Test\\\\Apple Braeburn', 1: 'Test\\\\Apple Crimson Snow', 2: 'Test\\\\Apple Golden 1', 3: 'Test\\\\Apple Granny Smith', 4: 'Test\\\\Apple Pink Lady', 5: 'Test\\\\Apple Red 1', 6: 'Test\\\\Apple Red Yellow 1', 7: 'Test\\\\Apricot', 8: 'Test\\\\Avocado', 9: 'Test\\\\Avocado ripe', 10: 'Test\\\\Banana', 11: 'Test\\\\Beetroot', 12: 'Test\\\\Blueberry', 13: 'Test\\\\Cantaloupe 1', 14: 'Test\\\\Cauliflower', 15: 'Test\\\\Cherry 1', 16: 'Test\\\\Chestnut', 17: 'Test\\\\Clementine', 18: 'Test\\\\Cocos', 19: 'Test\\\\Dates', 20: 'Test\\\\Eggplant', 21: 'Test\\\\Ginger Root', 22: 'Test\\\\Grape Blue', 23: 'Test\\\\Grape Pink', 24: 'Test\\\\Grape White', 25: 'Test\\\\Grapefruit White', 26: 'Test\\\\Hazelnut', 27: 'Test\\\\Huckleberry', 28: 'Test\\\\Kaki', 29: 'Test\\\\Kiwi', 30: 'Test\\\\Kumquats', 31: 'Test\\\\Lemon', 32: 'Test\\\\Limes', 33: 'Test\\\\Mandarine', 34: 'Test\\\\Mango', 35: 'Test\\\\Mango Red', 36: 'Test\\\\Mulberry', 37: 'Test\\\\Nectarine', 38: 'Test\\\\Nut Pecan', 39: 'Test\\\\Onion Red', 40: 'Test\\\\Onion White', 41: 'Test\\\\Orange', 42: 'Test\\\\Papaya', 43: 'Test\\\\Passion Fruit', 44: 'Test\\\\Peach', 45: 'Test\\\\Pear', 46: 'Test\\\\Pepper Green', 47: 'Test\\\\Pepper Red', 48: 'Test\\\\Pepper Yellow', 49: 'Test\\\\Pineapple', 50: 'Test\\\\Plum', 51: 'Test\\\\Pomegranate', 52: 'Test\\\\Potato Red', 53: 'Test\\\\Potato Sweet', 54: 'Test\\\\Potato White', 55: 'Test\\\\Raspberry', 56: 'Test\\\\Tangelo', 57: 'Test\\\\Tomato 1', 58: 'Test\\\\Tomato Cherry Red', 59: 'Test\\\\Walnut'}\n"
     ]
    }
   ],
   "source": [
    "# Label test set\n",
    "test_id = {v:k for k,v in enumerate(np.unique(test_label)) }\n",
    "test_target = {v:k for k,v in test_id.items() }\n",
    "print(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ... 59 59 59]\n",
      "(10102, 64, 64, 3) (10102,)\n"
     ]
    }
   ],
   "source": [
    "# Place test labels in a numpy array\n",
    "test_label_id = np.array([test_id[i] for i in test_label])\n",
    "print(test_label_id)\n",
    "print(test_img.shape, test_label_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29586, 64, 64, 3)\n",
      "(29586, 12288)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_img\n",
    "X_train_2D = X_train.reshape(train_img.shape[0],64*64*3)\n",
    "print(X_train.shape)\n",
    "print(X_train_2D.shape)\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "X_train = X_train/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10102, 64, 64, 3)\n",
      "(10102, 12288)\n"
     ]
    }
   ],
   "source": [
    "X_test = test_img\n",
    "X_test_2D = test_img.reshape(test_img.shape[0],64*64*3)\n",
    "print(X_test.shape)\n",
    "print(X_test_2D.shape)\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = label_id\n",
    "Y_test = test_label_id\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = keras.utils.to_categorical(Y_train, 60)\n",
    "Y_test = keras.utils.to_categorical(Y_test, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 16)        448       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                15420     \n",
      "=================================================================\n",
      "Total params: 638,364\n",
      "Trainable params: 637,884\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer\n",
    "model.add(Conv2D(16,(3,3),input_shape=(64,64,3),padding='same'))\n",
    "model.add(LeakyReLU(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same'))\n",
    "model.add(LeakyReLU(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same'))\n",
    "model.add(LeakyReLU(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same'))\n",
    "model.add(LeakyReLU(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(60, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adamax(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29586 samples, validate on 10102 samples\n",
      "Epoch 1/10\n",
      "29586/29586 [==============================] - 874s 30ms/step - loss: 1.3987 - accuracy: 0.6248 - val_loss: 18.2125 - val_accuracy: 0.0164\n",
      "Epoch 2/10\n",
      "29586/29586 [==============================] - 990s 33ms/step - loss: 0.3397 - accuracy: 0.8964 - val_loss: 7.6538 - val_accuracy: 0.0384\n",
      "Epoch 3/10\n",
      "29586/29586 [==============================] - 1115s 38ms/step - loss: 0.1706 - accuracy: 0.9487 - val_loss: 0.6715 - val_accuracy: 0.8101\n",
      "Epoch 4/10\n",
      "29586/29586 [==============================] - 1072s 36ms/step - loss: 0.1128 - accuracy: 0.9672 - val_loss: 0.3560 - val_accuracy: 0.8884\n",
      "Epoch 5/10\n",
      "29586/29586 [==============================] - 1019s 34ms/step - loss: 0.0808 - accuracy: 0.9769 - val_loss: 0.3023 - val_accuracy: 0.9122\n",
      "Epoch 6/10\n",
      "29586/29586 [==============================] - 1077s 36ms/step - loss: 0.0615 - accuracy: 0.9827 - val_loss: 0.1787 - val_accuracy: 0.9471\n",
      "Epoch 7/10\n",
      "29586/29586 [==============================] - 1237s 42ms/step - loss: 0.0464 - accuracy: 0.9866 - val_loss: 0.1538 - val_accuracy: 0.9466\n",
      "Epoch 8/10\n",
      "29586/29586 [==============================] - 1078s 36ms/step - loss: 0.0413 - accuracy: 0.9876 - val_loss: 1.1771 - val_accuracy: 0.8101\n",
      "Epoch 9/10\n",
      "29586/29586 [==============================] - 1079s 36ms/step - loss: 0.0336 - accuracy: 0.9907 - val_loss: 0.1520 - val_accuracy: 0.9582\n",
      "Epoch 10/10\n",
      "29586/29586 [==============================] - 1056s 36ms/step - loss: 0.0300 - accuracy: 0.9911 - val_loss: 0.2319 - val_accuracy: 0.9477\n",
      "Run time: 10705.92505645752\n",
      "Test loss: 0.23187635408114252\n",
      "Test accuracy: 0.9477331042289734\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "end_time = time.time()\n",
    "print('Run time:', end_time-start_time)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Convolutional Neural Networl model had about three with hours run time and an accuracy of almost 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29586 samples, validate on 10102 samples\n",
      "Epoch 1/5\n",
      " 2304/29586 [=>............................] - ETA: 98:39:32 - loss: 4.0895 - accuracy: 0.0234"
     ]
    }
   ],
   "source": [
    "# Embedding dimensions.\n",
    "row_hidden = 128\n",
    "col_hidden = 128\n",
    "\n",
    "row, col, pixel = X_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(60, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "start_time = time.time()\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "end_time = time.time()\n",
    "print('Run time:', end_time-start_time)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
