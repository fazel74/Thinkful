{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Test Basic Scraper & APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.everydaysexism.com',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "        # Get the URL of the previous page.\n",
    "        next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
    "        \n",
    "        # There are a LOT of pages here.  For our example, we'll just scrape the first 9.\n",
    "        # This finds the page number. The next segment of code prevents us from going beyond page 9.\n",
    "        pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
    "        \n",
    "        # Recursively call the spider to run on the next page, if it exists.\n",
    "        if next_page is not None and pagenum < 10:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            # Request the next page and recursively parse it the same way we did above\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'data.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n",
      "        date               name            tags  \\\n",
      "0 2019-06-02             Bessie              []   \n",
      "1 2019-06-02     Creepy old men  [Public space]   \n",
      "2 2019-06-02               Dumb  [Public space]   \n",
      "3 2019-06-02             London        [School]   \n",
      "4 2019-06-02  Oddball neighbour          [Home]   \n",
      "\n",
      "                                                text  \n",
      "0  [Ok I hope I don’t upset anyone but in really ...  \n",
      "1  [Married very old  friend of my dads thought m...  \n",
      "2  [A couple of years ago, \\nWe did a pub quiz an...  \n",
      "3  [Twenty years ago, \\nMy Muslim friend was marr...  \n",
      "4  [I have been emailed by a neighbours  husband ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data from all 9 pages\n",
    "ESSdf=pd.read_json('data.json', orient='records')\n",
    "print(ESSdf.shape)\n",
    "print(ESSdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94, 1)\n",
      "                         title\n",
      "89           Beyond the Fringe\n",
      "90                        Cunt\n",
      "91        List of Welsh people\n",
      "92  Downing College, Cambridge\n",
      "93                 Dan Aykroyd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data \n",
    "\n",
    "Monty=pd.read_json('PythonLinks.json', orient='records')\n",
    "print(Monty.shape)\n",
    "print(Monty.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Challenge\n",
    "Do a little scraping or API-calling of your own. Pick a new website and see what you can get out of it. Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.\n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)<br>\n",
    "2) Iterate over multiple pages/queries<br>\n",
    "3) Save the data to your computer<br>\n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest. Write up a report from scraping code to summary and share it with your mentor.\n",
    "\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# https://losangeles.craigslist.org/\n",
    "    \n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "\n",
    "class CLSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"CL\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    allowed_domains = [\"losangeles.craigslist.org\"]\n",
    "    start_urls = [\n",
    "        'https://losangeles.craigslist.org/search/apa?min_bedrooms=1&max_bedrooms=2&min_bathrooms=1&max_bathrooms=2&availabilityMode=0&sale_date=all+dates'\n",
    "    ]\n",
    "    \n",
    "    rules = (Rule (LinkExtractor(allow=(\"index\\d00\\.html\", ),restrict_xpaths=('//p[@id=\"nextpage\"]',)), callback=\"parse_items\", follow= True),)\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for posting in response.xpath('//p'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'title': posting.xpath('a[@class=\"result-title hdrlnk\"]/text()').extract_first(),\n",
    "                'date': posting.xpath('time[@class=\"result-date\"]/text()').extract_first(),\n",
    "                'price': posting.xpath('span/span[@class=\"result-price\"]/text()').extract_first()\n",
    "            }\n",
    "        \n",
    "        # follow next page links\n",
    "        next_page = response.xpath('.//a[@class=\"button next\"]/@href').extract()\n",
    "        if next_page:\n",
    "            next_href = next_page[0]\n",
    "            next_page_url = 'http://losangeles.craigslist.org' + next_href\n",
    "            request = scrapy.Request(url=next_page_url)\n",
    "            yield request  \n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'tempe_apt.json',  # Name our storage file.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcamp (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(CLSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3002, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>$4695</td>\n",
       "      <td>The Glendon: Westwood's premier Class A property!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>$1950</td>\n",
       "      <td>Covered Parking, Refrigerator, Microwave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>$2803</td>\n",
       "      <td>2 bedroom| Central A/C and Heat| Free Parking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>$1995</td>\n",
       "      <td>Newly renovated 1 bedroom apartment in Mar Vista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>$1899</td>\n",
       "      <td>Lovely 2 Bedroom Apartment… Waiting for You to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date  price                                              title\n",
       "0  Jan 29  $4695  The Glendon: Westwood's premier Class A property!\n",
       "1  Jan 29  $1950           Covered Parking, Refrigerator, Microwave\n",
       "2  Jan 29  $2803  2 bedroom| Central A/C and Heat| Free Parking ...\n",
       "3  Jan 29  $1995   Newly renovated 1 bedroom apartment in Mar Vista\n",
       "4  Jan 29  $1899  Lovely 2 Bedroom Apartment… Waiting for You to..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data \n",
    "df_apt = pd.read_json('tempe_apt.json')\n",
    "print(df_apt.shape)\n",
    "df_apt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically searched for apartment max 2 bedrooms min 1 bedroom & max 2 restrooms min 1 restrooms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date     object\n",
       "price    object\n",
       "title    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types\n",
    "df_apt.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>4695.0</td>\n",
       "      <td>The Glendon: Westwood's premier Class A property!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>Covered Parking, Refrigerator, Microwave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>2803.0</td>\n",
       "      <td>2 bedroom| Central A/C and Heat| Free Parking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Newly renovated 1 bedroom apartment in Mar Vista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 29</td>\n",
       "      <td>1899.0</td>\n",
       "      <td>Lovely 2 Bedroom Apartment… Waiting for You to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date   price                                              title\n",
       "0  Jan 29  4695.0  The Glendon: Westwood's premier Class A property!\n",
       "1  Jan 29  1950.0           Covered Parking, Refrigerator, Microwave\n",
       "2  Jan 29  2803.0  2 bedroom| Central A/C and Heat| Free Parking ...\n",
       "3  Jan 29  1995.0   Newly renovated 1 bedroom apartment in Mar Vista\n",
       "4  Jan 29  1899.0  Lovely 2 Bedroom Apartment… Waiting for You to..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "# Scrub the dollar sign away\n",
    "df_apt.price = df_apt.price.map(lambda x: None if x == None else int(re.sub('\\$', '', str(x))))\n",
    "df_apt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date      object\n",
       "price    float64\n",
       "title     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types\n",
    "df_apt.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total   Percent\n",
       "title      2  0.000666\n",
       "price      2  0.000666\n",
       "date       2  0.000666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "total_missing = df_apt.isnull().sum().sort_values(ascending=False) \n",
    "                    #find the total null data values in each column based on percentage of data points in each column \n",
    "\n",
    "percent_missing = (df_apt.isnull().sum()/df_apt.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "missing_data = pd.concat([total_missing, percent_missing], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df_apt = df_apt.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations & Statistical Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3000 entries, 0 to 2999\n",
      "Data columns (total 3 columns):\n",
      "date     3000 non-null object\n",
      "price    3000 non-null float64\n",
      "title    3000 non-null object\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 93.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_apt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(df_apt.price, 30)\n",
    "plt.xlabel('Apartment Prices')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Price of Los Angeles Apartments on Craigslist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Aparment Price: $3.239e+04\n",
      "\n",
      "Apartment Price Standard Deviation: +/-$1.44e+06\n",
      "\n",
      "Apartment Price Median: +/-$2192\n"
     ]
    }
   ],
   "source": [
    "print('Average Aparment Price: ${0:.4g}\\n'.format(df_apt.price.mean()))\n",
    "print('Apartment Price Standard Deviation: +/-${0:.3g}\\n'.format(df_apt.price.std()))\n",
    "print('Apartment Price Median: +/-${0:.4g}'.format(df_apt.price.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data that could be scraped was collected, totaling 3002 observations. This allowed for a confident estimate of the average price of max 2 min 1 bedroom & max 2 min 1 restroom apartments listed on the Los Angeles craigslist portal. Also the Scrapying worked well to follow the \"next page\" links. The visualization didn't go well probably because there are some outliers. Further analyses are always possible  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
